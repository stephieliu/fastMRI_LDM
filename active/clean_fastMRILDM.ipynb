{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ACTIVE] Personal Dataset creation\n",
    "\n",
    "# Import statements\n",
    "import fastmri\n",
    "from fastmri.data import transforms as T\n",
    "\n",
    "import h5py\n",
    "\n",
    "import os\n",
    "\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory for the data\n",
    "directory = \"../../kadotab/Datasets/16_chans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to visualize coils, to be used later\n",
    "def show_coils(data, slice_nums, cmap=None):\n",
    "    fig = plt.figure()\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.close(fig) # Close the figure window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to save and display coil images, to be used later\n",
    "def save_coils(path, filename, i, j, data, slice_nums, cmap=None):\n",
    "    fig = plt.figure()\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.savefig(path + filename + str(i) + \"_\" + str(j) + \".png\")\n",
    "        plt.close(fig) # Close the figure window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the kspace data\n",
    "\n",
    "# Create a csv file with all filenames for parsing through the dataset later\n",
    "# Open the file and truncate it (clears the file)\n",
    "f = open(\"fastMRI_filenames.csv\",'w+')\n",
    "\n",
    "# Write the filenames from the desired folder into the csv file\n",
    "w = csv.writer(f)\n",
    "for path, dirs, files in os.walk(\"../kadotab/Datasets/16_chans/train\"):\n",
    "    for filename in files:\n",
    "        w.writerow([filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the k-space for each individual coil per slice as an image file and save it in a new directory\n",
    "\n",
    "# Set the directories\n",
    "csvDir = 'fastMRI_filenames.csv'\n",
    "rootDir = '../kadotab/Datasets/16_chans/train'\n",
    "pathDir = 'fastMRI_kspace_images/'\n",
    "\n",
    "# Create an initial dataframe to read\n",
    "# frame = pd.read_csv(w)\n",
    "\n",
    "# Iterate through the dataframe and create png files for the data\n",
    "with open(csvDir, 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        # Access the first value in each row and set it as the directory to the kspace file\n",
    "        # Check that the correct file is being accessed\n",
    "        # print(row[0])\n",
    "        name = os.path.join(rootDir, row[0])\n",
    "\n",
    "        # Access the individual file\n",
    "        file = h5py.File(name)\n",
    "        \n",
    "        # Access the k-space numpy array\n",
    "        sample = file['kspace'][()]\n",
    "\n",
    "        # Want to save each coil per slice as an image, and this will be the new dataset\n",
    "        # There are 16 slices and 16 coils\n",
    "\n",
    "        # Iterate through each slice in the scan\n",
    "        count1 = 0 # Initialize counter variables for naming\n",
    "        # Take the i-th slice of the scan\n",
    "        for slice in sample:\n",
    "            # slice = sample[i] # Take the i-th slice of the scan\n",
    "            slice_kspace = T.to_tensor(slice) # Convert from numpy array to pytorch tensor\n",
    "            slice_image = fastmri.ifft2c(slice_kspace) # Apply Inverse Fourier Transform to get the complex image\n",
    "            slice_image_abs = fastmri.complex_abs(slice_image) # Compute absolute value to get a real image\n",
    "            count2 = 0 # Initialize counter variables for indexing\n",
    "            for coil in slice:\n",
    "                # Checkpoint: Show the j-th coil for the i-th slice\n",
    "                show_coils(slice_image_abs, [count2], cmap='gray')\n",
    "\n",
    "                # Save the coil images as \"ij.png\"\n",
    "                save_coils(pathDir, row[0], count1, count2, slice_image_abs, [count2], cmap='gray')\n",
    "                # Increment the counter by 1 to produce a new filename\n",
    "                count2+=1\n",
    "            count1+=1 # Increment the counter by 1 to produce a new filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the image data\n",
    "\n",
    "# Create a csv file with all filenames for parsing through the dataset later\n",
    "# Open the file and truncate it (clears the file)\n",
    "f1=open(\"fastMRI_images_filenames.csv\",'w+')\n",
    "\n",
    "# Write the filenames from the desired folder into the csv file\n",
    "w1=csv.writer(f1)\n",
    "for path, dirs, files in os.walk(\"fastMRI_kspace_images/\"):\n",
    "    for filename in files:\n",
    "        w.writerow([filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class fastMRICustomDataset(Dataset):\n",
    "    \"\"\"Custom fastMRI dataset for loading k-space data.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with filenames.\n",
    "            root_dir (string): Directory with all the fastMRI image data.\n",
    "            transform (callable, optional):  Transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read the individual file\n",
    "        data_name = os.path.join(self.root_dir,\n",
    "                                self.dataframe.iloc[idx, 0])\n",
    "        \n",
    "        # file = h5py.File(data_name)\n",
    "\n",
    "        # # Access the k-space numpy array\n",
    "        # sample = file['kspace'][()]\n",
    "\n",
    "        # # Apply toTensor transform to the data if requested\n",
    "        # sample = T.to_tensor(sample)\n",
    "\n",
    "        # Simply return the image\n",
    "        image = read_image(data_name)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to load the dataset of images\n",
    "\n",
    "# Set the directories\n",
    "csv_dir = 'fastMRI_images_filenames.csv'\n",
    "root_dir = 'fastMRI_kspace_images/'\n",
    "\n",
    "# Set up the dataset\n",
    "kspace_dataset = fastMRICustomDataset(\n",
    "    csv_file=csv_dir,\n",
    "    root_dir=root_dir,\n",
    ")\n",
    "\n",
    "# Check the dataset shape (passed)\n",
    "# sample = kspace_dataset[0]\n",
    "# print(sample.dtype)\n",
    "# print(sample.shape)\n",
    "# Output: torch.float32\n",
    "# torch.Size([16, 16, 640, 320, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the dataloader\n",
    "dataloader = DataLoader(kspace_dataset, batch_size=4, shuffle=True, num_workers=8)\n",
    "# The dataloader should now load coil images\n",
    "\n",
    "# NEW: Dataloader should load images\n",
    "# Check output data\n",
    "train_img = next(iter(dataloader))\n",
    "img = train_img[0].squeeze()\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Check dataloader data (passed)\n",
    "# print(dataloader.dataset[0].dtype)\n",
    "# print(dataloader.dataset[0].shape)\n",
    "# Output: torch.float32\n",
    "# torch.Size([16, 16, 640, 320, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access one k-space sample and show coils\n",
    "\n",
    "# Convert back from tensor to numpy array\n",
    "# kspace_nparray = T.tensor_to_complex_np(dataloader.dataset[0])\n",
    "\n",
    "# Choose the 8th slice\n",
    "# slice_kspace = kspace_nparray[8]\n",
    "\n",
    "# Show coils 0, 5, and 10 (passed, see OneNote for output)\n",
    "# show_coils(np.log(np.abs(slice_kspace) + 1e-9), [0, 5, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building the Diffusion Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Google Colab tutorial: https://colab.research.google.com/drive/1sjy9odlSSy0RBVgMTgP7s99NXsqglsUL?usp=sharing#scrollTo=hqcoJ8ZlXE1i\n",
    "\n",
    "## Step 1: The forward process = Noise scheduler\n",
    "\n",
    "We first need to build the inputs for our model, which are more and more noisy images. Instead of doing this sequentially, we can use the closed form provided in the papers to calculate the image for any of the timesteps individually.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- The noise-levels/variances can be pre-computed\n",
    "- There are different types of variance schedules\n",
    "- We can sample each timestep image independently (Sums of Gaussians is also Gaussian)\n",
    "- No model is needed in this forward step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Simple LDM Code (not functional yet)\n",
    "\n",
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    \"\"\" \n",
    "    Returns a specific index t of a passed list of values vals\n",
    "    while considering the batch dimension.\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device=\"cpu\"):\n",
    "    \"\"\" \n",
    "    Takes an image and a timestep as input and \n",
    "    returns the noisy version of it\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
    "    )\n",
    "    # mean + variance\n",
    "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
    "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
    "\n",
    "\n",
    "# Define beta schedule\n",
    "T = 300\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it on the dataset\n",
    "from torchvision import transforms \n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def load_transformed_dataset():\n",
    "    data_transforms = [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(), # Scales data into [0,1] \n",
    "        transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1] \n",
    "    ]\n",
    "    data_transform = transforms.Compose(data_transforms)\n",
    "\n",
    "    train = torchvision.datasets.StanfordCars(root=\".\", download=True, \n",
    "                                         transform=data_transform)\n",
    "\n",
    "    test = torchvision.datasets.StanfordCars(root=\".\", download=True, \n",
    "                                         transform=data_transform, split='test')\n",
    "    return torch.utils.data.ConcatDataset([train, test])\n",
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "        transforms.Lambda(lambda t: t * 255.),\n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "    # Take first image of batch\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0, :, :, :] \n",
    "    plt.imshow(reverse_transforms(image))\n",
    "\n",
    "data = load_transformed_dataset()\n",
    "dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate forward diffusion\n",
    "image = next(iter(dataloader))[0]\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.axis('off')\n",
    "num_images = 10\n",
    "stepsize = int(T/num_images)\n",
    "\n",
    "for idx in range(0, T, stepsize):\n",
    "    t = torch.Tensor([idx]).type(torch.int64)\n",
    "    plt.subplot(1, num_images+1, int(idx/stepsize) + 1)\n",
    "    img, noise = forward_diffusion_sample(image, t)\n",
    "    show_tensor_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The backward process = U-Net\n",
    "\n",
    "For a great introduction to UNets, have a look at this post: https://amaarora.github.io/2020/09/13/unet.html.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- We use a simple form of a UNet for to predict the noise in the image\n",
    "- The input is a noisy image, the ouput the noise in the image\n",
    "- Because the parameters are shared accross time, we need to tell the network in which timestep we are\n",
    "- The Timestep is encoded by the transformer Sinusoidal Embedding\n",
    "- We output one single value (mean), because the variance is fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t, ):\n",
    "        # First Conv\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        # Second Conv\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        # Down or Upsample\n",
    "        return self.transform(h)\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        # TODO: Double check the ordering here\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SimpleUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified variant of the Unet architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_channels = 3\n",
    "        down_channels = (64, 128, 256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256, 128, 64)\n",
    "        out_dim = 3 \n",
    "        time_emb_dim = 32\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "                nn.Linear(time_emb_dim, time_emb_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        # Downsample\n",
    "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], \\\n",
    "                                    time_emb_dim) \\\n",
    "                    for i in range(len(down_channels)-1)])\n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], \\\n",
    "                                        time_emb_dim, up=True) \\\n",
    "                    for i in range(len(up_channels)-1)])\n",
    "        \n",
    "        # Edit: Corrected a bug found by Jakub C (see YouTube comment)\n",
    "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        # Embedd time\n",
    "        t = self.time_mlp(timestep)\n",
    "        # Initial conv\n",
    "        x = self.conv0(x)\n",
    "        # Unet\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            # Add residual x as additional channels\n",
    "            x = torch.cat((x, residual_x), dim=1)           \n",
    "            x = up(x, t)\n",
    "        return self.output(x)\n",
    "\n",
    "model = SimpleUnet()\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further improvements that can be implemented:**\n",
    "\n",
    "- Residual connections\n",
    "- Different activation functions like SiLU, GWLU, ...\n",
    "- BatchNormalization\n",
    "- GroupNormalization\n",
    "- Attention\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The loss\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- After some maths we end up with a very simple loss function\n",
    "- There are other possible choices like L2 loss ect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, t):\n",
    "    x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
    "    noise_pred = model(x_noisy, t)\n",
    "    return F.l1_loss(noise, noise_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "- Without adding @torch.no_grad() we quickly run out of memory, because pytorch tacks all the previous images for gradient calculation\n",
    "- Because we pre-calculated the noise variances for the forward pass, we also have to use them when we sequentially perform the backward process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_timestep(x, t):\n",
    "    \"\"\"\n",
    "    Calls the model to predict the noise in the image and returns \n",
    "    the denoised image. \n",
    "    Applies noise to this image, if we are not in the last step yet.\n",
    "    \"\"\"\n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "    \n",
    "    if t == 0:\n",
    "        # As pointed out by Luis Pereira (see YouTube comment)\n",
    "        # The t's are offset from the t's in the paper\n",
    "        return model_mean\n",
    "    else:\n",
    "        noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_plot_image():\n",
    "    # Sample noise\n",
    "    img_size = IMG_SIZE\n",
    "    img = torch.randn((1, 3, img_size, img_size), device=device)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.axis('off')\n",
    "    num_images = 10\n",
    "    stepsize = int(T/num_images)\n",
    "\n",
    "    for i in range(0,T)[::-1]:\n",
    "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "        img = sample_timestep(img, t)\n",
    "        # Edit: This is to maintain the natural range of the distribution\n",
    "        img = torch.clamp(img, -1.0, 1.0)\n",
    "        if i % stepsize == 0:\n",
    "            plt.subplot(1, num_images, int(i/stepsize)+1)\n",
    "            show_tensor_image(img.detach().cpu())\n",
    "    plt.show()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100 # Try more!\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
    "      loss = get_loss(model, batch[0], t)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if epoch % 5 == 0 and step == 0:\n",
    "        print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "        sample_plot_image()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
